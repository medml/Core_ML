{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_Med_LogR.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPK23rfs8Ge6rFZmq2F3IGc"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Dl7NLeqI6PdN","colab_type":"text"},"source":["# **Logistic Regression**"]},{"cell_type":"markdown","metadata":{"id":"saP9tpE-AnMA","colab_type":"text"},"source":["## **Overview**\n","\n","So far, you've learned about linear regression, where you built a model to predict a continuous linear relationship between two variables. In that case, the outcome variable, blood glucose, is continuous and can be modeled well by a line. \n","\n","But what if your outcome variable is instead categorical, like a disease diagnosis? \n","\n","###The solution\n","\n","In data science (particularly in scikit-learn), we assign numerical values (like 1 and 0) to each categorical variable (like 'Yes Diabetes' and 'No Diabetes').\n","\n","If we tried to plot a linear regression model with this disease outcome as the dependent variable, you'd notice something funny.\n","\n","We only have *two* possible outcome values, 1 and 0, but the linear regression function that is plotted extends infinitely.\n","\n","So is there a better way to model our relationship?\n","\n","The logistic function outputs a sigmoidal curve that *squeezes* outputs to between 0 and 1.\n","\n","$f(x) = \\frac{1}{1+e^{-x}}$\n","\n","Now, by inputting our linear relationship model into the logistic function, we've generated a function that we can interpret as a *classifier*. (Note: despite its name, logistic regression, is actually a classifer.\n","\n","By default, any predicted value on our logistic curve above y=0.5 would be *classified* as a member of the positive class (i.e. 'Yes Diabetes') and any predicted value below y=0.5 would be *classified* as a member of the negative class (i.e. 'No Diabetes').\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mAC5Zc-_RigF","colab_type":"text"},"source":["### Code Setup"]},{"cell_type":"code","metadata":{"id":"xJei4K3ERwo1","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","#importing the LogisticRegression model from scikit-learn\n","from sklearn.linear_model import LogisticRegression\n","\n","#importing a built-in function for automatically creating train and test sets from your dataset\n","from sklearn.model_selection import train_test_split\n","\n","#importing an accuracy metric to evaluate the trained model\n","from sklearn.metrics import accuracy_score"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pFn8l34tUsHP","colab_type":"text"},"source":["### Dataset Import"]},{"cell_type":"markdown","metadata":{"id":"yFxurre7VJEC","colab_type":"text"},"source":["In this tutorial, we're going to use an existing dataset downloaded from Kaggle (a popular data science/machine learning competition website). The [dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database) itself was provided by the National Institute of Diabetes and Digestive and Kidney Diseases.\n","\n","The dataset includes 767 individuals and 8 associated features (e.g. age, BMI, 2 hour serum insulin, and others). The last column includes the diagnosis that the investigators provided for the individual's diabetic status and is '1' if they have diabetes or '0' if not."]},{"cell_type":"code","metadata":{"id":"MNywPu5oUz4C","colab_type":"code","colab":{}},"source":["url = 'https://raw.githubusercontent.com/amurugan19/medmldatasets/master/diabetes.csv'\n","\n","#importing the csv file into a dataframe\n","data = pd.read_csv(url)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7VpDyFl8a9gQ","colab_type":"text"},"source":["####Train/Test Split\n","\n","The next step is to generate a train and test set for our model from our original dataset. This can be done manually, but luckily, scikit-learn includes a handy function, called *train_test_split*, that will automatically split your original dataset."]},{"cell_type":"code","metadata":{"id":"k4g7aagNbgDG","colab_type":"code","colab":{}},"source":["#creating a dataframe variable called X to store our input features (e.g. age, BMI, etc.)\n","X = data.iloc[1:,0:-1]\n","\n","#separate dataframe variable called y to store our labels (e.g. 1 and 0 for Y or N diabetes diagnosis\n","y = data.iloc[1:,-1]\n","\n","#using a built-in function to generate the new train and test arrays\n","X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MphhtqH9cZyJ","colab_type":"text"},"source":["###Model Building\n","\n","At this point, we've successfully created our train and test sets. Now it's time to build our logistic regression model! "]},{"cell_type":"code","metadata":{"id":"_PQodlFvdm8l","colab_type":"code","outputId":"f477e6ae-95ea-4b36-e49f-7fd9ca96521f","executionInfo":{"status":"ok","timestamp":1586714775087,"user_tz":300,"elapsed":731,"user":{"displayName":"Avinash Murugan","photoUrl":"","userId":"08189905804220841502"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["model = LogisticRegression(max_iter=500);\n","\n","#training our model using our training inputs (X) and training labels (y)\n","model.fit(X_train, y_train)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"mCmllSVxh763","colab_type":"text"},"source":["Since our model has approximately *learned* the relationship between our inputs (like age, BMI, and 2 hour serum insulin level,etc.) and an individual's diabetes status in our training set, we can now use the model to *predict* the diabetes status for the individuals in our test set!\n"]},{"cell_type":"code","metadata":{"id":"0M0pVnyCimZY","colab_type":"code","outputId":"e62fc4f2-61b1-4cd0-ef73-588e77fe05c9","executionInfo":{"status":"ok","timestamp":1586714775089,"user_tz":300,"elapsed":720,"user":{"displayName":"Avinash Murugan","photoUrl":"","userId":"08189905804220841502"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["#creating a new dataframe to store our model's novel predictions\n","y_pred = model.predict(X_test)\n","\n","#printing our predictions \n","print(y_pred)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1\n"," 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0\n"," 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0\n"," 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1\n"," 1 0 0 0 1 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Yal4xUOPjOoP","colab_type":"text"},"source":["As you can see in the previous cell's output, our model has generated predictions for each patient in our test set (with 1 corresponding to 'Diabetes' and 0 corresponding to 'No Diabetes'). \n","\n","Unfortunately, plotting and visualizing this model is non-trivial, since our dataset contained 8 input variables (meaning that our data and model exist in an *8-dimensional space* (and most of us can't really visualize anything beyond a x-y-z axis for a 3-dimensional space)!\n","\n","However, we can still evaluate the accuracy of our model's predictions against the *ground truth* values (i.e. each individual's actual diabetes diagnosis) provided beforehand by the study investigators. "]},{"cell_type":"code","metadata":{"id":"hsXHkJcNkA9U","colab_type":"code","outputId":"10f526a3-6407-4939-b363-f64e78d56eaa","executionInfo":{"status":"ok","timestamp":1586714775090,"user_tz":300,"elapsed":710,"user":{"displayName":"Avinash Murugan","photoUrl":"","userId":"08189905804220841502"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#this is one of many pre-written functions for evaluation in the scikit metrics module\n","score = accuracy_score(y_test, y_pred)\n","\n","print(\"Logistic Regression Accuracy: \", score)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Logistic Regression Accuracy:  0.7402597402597403\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CUhSiQiDkj3B","colab_type":"text"},"source":["And that's all the code you need to generate a complete Logistic Regression binary classifier! For next steps, there are [various model parameters](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) that you can customize to improve performance for your classification task. Using your same dataset, can also try other classifier models like Random Forests or K-nearest-neighbors that we'll discuss elsewhere.\n"]},{"cell_type":"markdown","metadata":{"id":"zf1PQ0sMMWZp","colab_type":"text"},"source":["##Multi-Class Classification\n","\n","In the previous example, we used a dataset where the outcome measure was *binary*, that is, there were only two possibilities: diabetes or no diabetes. But what if you have more than two outcome classes? Imagine a neurology classification task where the outcomes include 3 type of bleeds: epidural hematoma, subdural hematoma, and subarachnoid hematoma. This type of task then becomes a *multi-class* (here, a 3 class) classification. \n","\n","Multi-class classification can be done using a \"one vs. rest\" approach for Logistic Regression, but it becomes a bit more complicated.\n","\n","Sequentally, each one of the three classes above is classified against the union of the other classes (i.e. epidural vs. {subdural examples $\\cup$ subarachnoid examples}; this effectively transforms our multi-class classification task into *many* binary classification tasks. Then, many separate logistic regression models are trained on each of these \"binary\" classification tasks.\n","\n","Alternately, you may choose to use other classification models like [*k nearest neighbors*](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification), which inherently are suited to multi-class classification.\n","\n","The implementation details are beyond the scope of this tutorial, but the scikit-learn documentation as well as Medium tutorials are a good next step.\n"]}]}